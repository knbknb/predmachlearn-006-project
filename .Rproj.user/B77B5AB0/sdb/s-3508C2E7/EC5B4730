{
    "contents" : "---\ntitle       : Predicting with trees\nsubtitle    : \nauthor      : Jeffrey Leek\njob         : Johns Hopkins Bloomberg School of Public Health\nlogo        : bloomberg_shield.png\nframework   : io2012        # {io2012, html5slides, shower, dzslides, ...}\nhighlighter : highlight.js  # {highlight.js, prettify, highlight}\nhitheme     : tomorrow   # \nurl:\n  lib: ../../librariesNew\n  assets: ../../assets\nwidgets     : [mathjax]            # {mathjax, quiz, bootstrap}\nmode        : selfcontained # {standalone, draft}\n---\n\n\n```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}\n# make this an external chunk that can be included in any file\noptions(width = 100)\nopts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache=TRUE, cache.path = '.cache/', fig.path = 'fig/')\n\noptions(xtable.type = 'html')\nknit_hooks$set(inline = function(x) {\n  if(is.numeric(x)) {\n    round(x, getOption('digits'))\n  } else {\n    paste(as.character(x), collapse = ', ')\n  }\n})\nknit_hooks$set(plot = knitr:::hook_plot_html)\n```\n\n\n## Key ideas\n\n* Iteratively split variables into groups\n* Evaluate \"homogeneity\" within each group\n* Split again if necessary\n\n__Pros__:\n\n* Easy to interpret\n* Better performance in nonlinear settings\n\n__Cons__:\n\n* Without pruning/cross-validation can lead to overfitting\n* Harder to estimate uncertainty\n* Results may be variable\n\n\n---\n\n## Example Tree\n\n<img class=center src=../../assets/img/08_PredictionAndMachineLearning/obamaTree.png height=450>\n\n[http://graphics8.nytimes.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg](http://graphics8.nytimes.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg)\n\n---\n\n## Basic algorithm\n\n1. Start with all variables in one group\n2. Find the variable/split that best separates the outcomes\n3. Divide the data into two groups (\"leaves\") on that split (\"node\")\n4. Within each split, find the best variable/split that separates the outcomes\n5. Continue until the groups are too small or sufficiently \"pure\"\n\n\n---\n\n## Measures of impurity\n\n$$\\hat{p}_{mk} = \\frac{1}{N_m}\\sum_{x_i\\; in \\; Leaf \\; m}\\mathbb{1}(y_i = k)$$\n\n__Misclassification Error__: \n$$ 1 - \\hat{p}_{m k(m)}; k(m) = {\\rm most; common; k}$$ \n* 0 = perfect purity\n* 0.5 = no purity\n\n__Gini index__:\n$$ \\sum_{k \\neq k'} \\hat{p}_{mk} \\times \\hat{p}_{mk'} = \\sum_{k=1}^K \\hat{p}_{mk}(1-\\hat{p}_{mk}) = 1 - \\sum_{k=1}^K p_{mk}^2$$\n\n* 0 = perfect purity\n* 0.5 = no purity\n\nhttp://en.wikipedia.org/wiki/Decision_tree_learning\n\n---\n\n## Measures of impurity\n\n__Deviance/information gain__:\n\n$$ -\\sum_{k=1}^K \\hat{p}_{mk} \\log_2\\hat{p}_{mk} $$\n* 0 = perfect purity\n* 1 = no purity\n\nhttp://en.wikipedia.org/wiki/Decision_tree_learning\n\n\n--- &twocol w1:50% w2:50%\n## Measures of impurity\n\n*** =left\n\n```{r leftplot,fig.height=3,fig.width=4,echo=FALSE,fig.align=\"center\"}\npar(mar=c(0,0,0,0)); set.seed(1234); x = rep(1:4,each=4); y = rep(1:4,4)\nplot(x,y,xaxt=\"n\",yaxt=\"n\",cex=3,col=c(rep(\"blue\",15),rep(\"red\",1)),pch=19)\n```\n\n* __Misclassification:__ $1/16 = 0.06$\n* __Gini:__ $1 - [(1/16)^2 + (15/16)^2] = 0.12$\n* __Information:__$-[1/16 \\times log2(1/16) + 15/16 \\times log2(15/16)] = 0.34$\n\n*** =right\n\n```{r,dependson=\"leftplot\",fig.height=3,fig.width=4,echo=FALSE,fig.align=\"center\"}\npar(mar=c(0,0,0,0)); \nplot(x,y,xaxt=\"n\",yaxt=\"n\",cex=3,col=c(rep(\"blue\",8),rep(\"red\",8)),pch=19)\n```\n\n* __Misclassification:__ $8/16 = 0.5$\n* __Gini:__ $1 - [(8/16)^2 + (8/16)^2] = 0.5$\n* __Information:__$-[1/16 \\times log2(1/16) + 15/16 \\times log2(15/16)] = 1$\n\n\n\n\n---\n\n## Example: Iris Data\n\n```{r iris, cache=TRUE}\ndata(iris); library(ggplot2)\nnames(iris)\ntable(iris$Species)\n```\n\n\n---\n\n## Create training and test sets\n\n```{r trainingTest, dependson=\"iris\",cache=TRUE}\ninTrain <- createDataPartition(y=iris$Species,\n                              p=0.7, list=FALSE)\ntraining <- iris[inTrain,]\ntesting <- iris[-inTrain,]\ndim(training); dim(testing)\n```\n\n\n---\n\n## Iris petal widths/sepal width\n\n```{r, dependson=\"trainingTest\",fig.height=4,fig.width=6}\nqplot(Petal.Width,Sepal.Width,colour=Species,data=training)\n```\n\n\n---\n\n## Iris petal widths/sepal width\n\n```{r createTree, dependson=\"trainingTest\", cache=TRUE}\nlibrary(caret)\nmodFit <- train(Species ~ .,method=\"rpart\",data=training)\nprint(modFit$finalModel)\n```\n\n---\n\n## Plot tree\n\n```{r, dependson=\"createTree\", fig.height=4.5, fig.width=4.5}\nplot(modFit$finalModel, uniform=TRUE, \n      main=\"Classification Tree\")\ntext(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)\n```\n\n\n---\n\n## Prettier plots\n\n```{r, dependson=\"createTree\", fig.height=4.5, fig.width=4.5}\nlibrary(rattle)\nfancyRpartPlot(modFit$finalModel)\n```\n\n---\n\n## Predicting new values\n\n```{r newdata, dependson=\"createTree\", fig.height=4.5, fig.width=4.5, cache=TRUE}\npredict(modFit,newdata=testing)\n```\n\n---\n\n## Notes and further resources\n\n* Classification trees are non-linear models\n  * They use interactions between variables\n  * Data transformations may be less important (monotone transformations)\n  * Trees can also be used for regression problems (continuous outcome)\n* Note that there are multiple tree building options\nin R both in the caret package - [party](http://cran.r-project.org/web/packages/party/index.html), [rpart](http://cran.r-project.org/web/packages/rpart/index.html) and out of the caret package - [tree](http://cran.r-project.org/web/packages/tree/index.html)\n* [Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/)\n* [Elements of Statistical Learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)\n* [Classification and regression trees](http://www.amazon.com/Classification-Regression-Trees-Leo-Breiman/dp/0412048418)\n\n\n",
    "created" : 1414013181538.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "213890128",
    "id" : "EC5B4730",
    "lastKnownWriteTime" : 1407963818,
    "path" : "~/Documents/coursera/datascience/bcaffo-github-io/courses/08_PracticalMachineLearning/019predictingWithTrees/index.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}